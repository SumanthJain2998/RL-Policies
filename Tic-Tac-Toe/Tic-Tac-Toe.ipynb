{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Tic-Tac-Toe\n",
        "\n",
        "Using Reinforcement algorithm train an agent to play tic-tac-toe.   \n",
        "\n",
        "\n",
        "*   Firstly we'll train 2 agents to play against each other and save their policy\n",
        "\n",
        "*   Use the policy to play against human\n",
        "\n",
        "refernce:https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E_C1b_smUpQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###State Setting\n",
        "\n",
        "We need a \"State\" class to act as both *board* and *judger*. It has functions recording board state of both players and update state when either player takes an action. Meanwhile, it is able to judge the end of game and give reward to players accordingly."
      ],
      "metadata": {
        "id": "PFl2WdeUVtRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/RL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nSKa-dHwNsU",
        "outputId": "deb951d4-fb38-4eca-fd4a-efa595aa9cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/RL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "1cWvHUWoVszP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXS7VuIlTcS_"
      },
      "outputs": [],
      "source": [
        "# Define shape of the board\n",
        "\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLUMNS=3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To formulate this reinforcement learning problem, the most important thing is to be clear about the 3 major components — ***state, action, and reward***. The state of this game is the board state of both the agent and its opponent, so we will initialise a 3x3 board with zeros indicating available positions and ***update positions with 1 if player 1 takes a move and -1 if player 2 takes a move***. The action is what positions a player can choose based on the current board state. ***Reward is between 0 and 1 and is only given at the end of the game***"
      ],
      "metadata": {
        "id": "s9nIIxQ8YHen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the class state that acts as board and judger\n",
        "\n",
        "class State:\n",
        "  def __init__(self,p1,p2):\n",
        "    self.board = np.zeros((BOARD_ROWS,BOARD_COLUMNS)) # initialize the board with zeros\n",
        "    self.p1 = p1                                      # player 1(agent1)\n",
        "    self.p2 = p2                                      # player 2(agent2)\n",
        "    self.isEnd = False\n",
        "    self.boardHash = None\n",
        "    # init p1 plays first\n",
        "    self.playerSymbol = 1\n",
        "\n",
        "#get the unique hash of current board\n",
        "#The getHash function hashes the current board state so that it can be stored in the state-value dictionary.\n",
        "\n",
        "  def getHash(self):\n",
        "    self.boardHash = str(self.board.reshape(BOARD_COLUMNS*BOARD_ROWS))\n",
        "    return self.boardHash\n",
        "\n",
        "#When a player takes an action, its corresponding symbol will be filled in the board.\n",
        "#And after the state being updated, the board will also update the current vacant positions on the board\n",
        "#and feed it back to the next player in turn.\n",
        "\n",
        "  def availablePositions(self):\n",
        "    positions = []\n",
        "    for i in range (BOARD_ROWS):\n",
        "      for j in range (BOARD_COLUMNS):\n",
        "        if self.board[i,j] ==0:\n",
        "          positions.append((i,j))                     # need to be a tuple\n",
        "    return positions\n",
        "\n",
        "\n",
        "  def updateState(self,positions):\n",
        "    self.board[positions] = self.playerSymbol\n",
        "    #switch to another player\n",
        "    self.playerSymbol = -1 if self.playerSymbol==1 else 1\n",
        "\n",
        "#Check winner\n",
        "#   After each action being taken by the player,\n",
        "#   we need a function to continuously check if the game has ended and if end,\n",
        "#   to judge the winner of the game and give reward to both players.\n",
        "#\n",
        "#   The winner function checks sum of rows, columns and diagonals,\n",
        "#   and return 1 if p1 wins, -1 if p2 wins, 0 if draw and None if the game is not yet ended.\n",
        "#   At the end of game, 1 is rewarded to winner and 0 to loser.\n",
        "#   One thing to notice is that we consider draw is also a bad end, so we give our agent p1 0.1 reward even the game is tie\n",
        "#   (one can try out different reward to see how the agents act)\n",
        "\n",
        "  def winner(self):\n",
        "    #row\n",
        "    for i in range(BOARD_ROWS):\n",
        "      if sum(self.board[i,:])==3:\n",
        "        self.isEnd = True\n",
        "        return 1\n",
        "      if sum(self.board[i,:])==-3:\n",
        "        self.isEnd = True\n",
        "        return -1\n",
        "\n",
        "    #col\n",
        "    for i in range(BOARD_COLUMNS):\n",
        "      if sum(self.board[:,i])== 3:\n",
        "        self.isEnd = True\n",
        "        return 1\n",
        "      if sum(self.board[:,i])==-3:\n",
        "        self.isEnd = True\n",
        "        return -1\n",
        "\n",
        "    #diagonal\n",
        "    diag_sum1 = sum([self.board[i,i] for i in range(BOARD_COLUMNS)])\n",
        "    diag_sum2 = sum([self.board[i, BOARD_COLUMNS-i-1] for i in range(BOARD_COLUMNS)])\n",
        "    diag_sum = max(abs(diag_sum1), abs(diag_sum2))\n",
        "    if diag_sum ==3:\n",
        "      self.isEnd = True\n",
        "      if  diag_sum1 == 3 or diag_sum2 ==3:\n",
        "        return 1\n",
        "      else:\n",
        "        return -1\n",
        "\n",
        "    #tie\n",
        "    # no available positions\n",
        "\n",
        "    if len(self.availablePositions())==0:\n",
        "      self.isEnd = True\n",
        "      return 0\n",
        "\n",
        "    #not end\n",
        "    self.isEnd = False\n",
        "    return None\n",
        "\n",
        "# only when the game ends\n",
        "  def giveReward(self):\n",
        "    result  = self.winner()\n",
        "\n",
        "    #backpropagate the reward\n",
        "    if result ==1:\n",
        "      self.p1.feedReward(1)\n",
        "      self.p2.feedReward(0)\n",
        "    elif result==-1:\n",
        "      self.p1.feedReward(0)\n",
        "      self.p2.feedReward(1)\n",
        "    else:\n",
        "      self.p1.feedReward(0.1)\n",
        "      self.p2.feedReward(0.5)\n",
        "\n",
        "#board reset\n",
        "  def reset(self):\n",
        "    self.board = np.zeros((BOARD_ROWS, BOARD_COLUMNS))\n",
        "    self.boardHash = None\n",
        "    self.isEnd = True\n",
        "    self.playerSymbol  =1\n",
        "\n",
        "  def play(self, rounds=100):\n",
        "    for i in range(rounds):\n",
        "      #if i % 100 ==0:\n",
        "      #  print(\"Rounds {}\".format(i))\n",
        "      while not self.isEnd:\n",
        "        #player 1\n",
        "        positions = self.availablePositions()\n",
        "        p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
        "        # take action and update board state\n",
        "        self.updateState(p1_action)\n",
        "        board_hash = self.getHash()\n",
        "        self.p1.addState(board_hash)\n",
        "        print('agent_1:',self.p1.states_value)\n",
        "        #check board status if it is end\n",
        "\n",
        "        win = self.winner()\n",
        "        if win is not None:\n",
        "          self.giveReward()\n",
        "          self.p1.reset()\n",
        "          self.p2.reset()\n",
        "          self.reset()\n",
        "          break\n",
        "\n",
        "\n",
        "        else:\n",
        "          #player 2\n",
        "          postions = self.availablePositions()\n",
        "          p2_action = self.p2.chooseAction(positions,self.board, self.playerSymbol)\n",
        "          self.updateState(p2_action)\n",
        "          board_hash = self.getHash()\n",
        "          self.p2.addState(board_hash)\n",
        "          print('agent_2:',self.p2.states_value)\n",
        "\n",
        "\n",
        "          win = self.winner()\n",
        "          if win is not None:\n",
        "            self.giveReward()\n",
        "            self.p1.reset()\n",
        "            self.p2.reset()\n",
        "            self.reset()\n",
        "            break\n",
        "    self.p1.savePolicy()\n",
        "    self.p2.savePolicy()\n",
        "\n",
        "#play with human\n",
        "  def play2(self):\n",
        "    while not self.isEnd:\n",
        "      #player1\n",
        "      positions = self.availablePositions()\n",
        "      p1_action = self.p1.chooseAction(positions,self.board,self.playerSymbol)\n",
        "      #take action and update the board\n",
        "      self.updateState(p1_action)\n",
        "      self.showBoard()\n",
        "      win = self.winner()\n",
        "      if win is not None:\n",
        "        if win==1:\n",
        "          print(self.p1.name,'wins!')\n",
        "        else:\n",
        "          print('tie!')\n",
        "        self.reset()\n",
        "        break\n",
        "      else:\n",
        "        positions  = self.availablePositions()\n",
        "        p2_action = self.p2.chooseAction(positions)\n",
        "\n",
        "        self.updateState(p2_action)\n",
        "        self.showBoard()\n",
        "        win = self.winner()\n",
        "        if win is not None:\n",
        "          if win==-1:\n",
        "            print(self.p2.name, 'wins!')\n",
        "          else:\n",
        "            print('tie')\n",
        "          self.reset\n",
        "          break\n",
        "\n",
        "  def showBoard(self):\n",
        "    #p1:x p2:o\n",
        "    for i in range(0,BOARD_ROWS):\n",
        "      print('-------------')\n",
        "      out = '|'\n",
        "      for j in range(0, BOARD_COLUMNS):\n",
        "        if self.board[i,j]==1:\n",
        "          token = 'x'\n",
        "        if self.board[i,j]==-1:\n",
        "          token = 'o'\n",
        "        if self.board[i,j]==0:\n",
        "          token = ' '\n",
        "        out += token + ' | '\n",
        "      print(out)\n",
        "    print('-------------')"
      ],
      "metadata": {
        "id": "erW1fRQSWfHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Player:\n",
        "  '''In the init function, we keep track of all positions the player's\n",
        "   been taken during each game in a list self.states and update the corresponding states in self.states_value dict.\n",
        "   In terms of choosing action, we use ϵ-greedy method to balance between exploration and exploitation.\n",
        "   Here we set exp_rate=0.3 , which means ϵ=0.3 , so 70% of the time our agent will take greedy action,\n",
        "   which is choosing action based on current estimation of states-value,\n",
        "   and 30% of the time our agent will take random action\n",
        "   '''\n",
        "  def __init__(self,name,exp_rate=0.1):\n",
        "    self.name = name\n",
        "    self.states = []\n",
        "    self.lr = 0.01\n",
        "    self.exp_rate = exp_rate\n",
        "    self.decay_gamma = 0.9\n",
        "    self.states_value = {}\n",
        "\n",
        "  def getHash(self, board):\n",
        "    boardHash = str(board.reshape(BOARD_COLUMNS*BOARD_ROWS))\n",
        "    return boardHash\n",
        "\n",
        "  def chooseAction(self, positions, current_board, symbol):\n",
        "    if np.random.uniform(0,1) <= self.exp_rate:\n",
        "      idx = np.random.choice(len(positions))\n",
        "      action = positions[idx]\n",
        "    else:\n",
        "      value_max = -999\n",
        "      for p in positions:\n",
        "        next_board = current_board.copy()\n",
        "        next_board[p] = symbol\n",
        "        next_boardHash = self.getHash(next_board)\n",
        "        value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
        "        #print('value', value)\n",
        "        if value> value_max:\n",
        "          value_max = value\n",
        "          action = p\n",
        "\n",
        "      #print(\"{} takes action {}\".format(self.name, action))\n",
        "      return action\n",
        "\n",
        "\n",
        "    #append a hash state\n",
        "  def addState(self, state):\n",
        "    self.states.append(state)\n",
        "\n",
        "    #at the end of the game, back propagate and update states value\n",
        "    '''the updated value of state t equals the current value of state t\n",
        "    adding the difference between the value of next state and the value of current state,\n",
        "    which is multiplied by a learning rate α (Given the reward of intermediate state is 0).\n",
        "    '''\n",
        "  def feedReward(self, reward):\n",
        "    for st in reversed(self.states):\n",
        "      if self.states_value.get(st) is None:\n",
        "        self.states_value[st] = 0\n",
        "      self.states_value[st] +=self.lr*(self.decay_gamma*reward-self.states_value[st])\n",
        "      reward = self.states_value[st]\n",
        "\n",
        "    '''the positions of each game is stored in self.states\n",
        "    and when the agent reach the end of the game,\n",
        "    the estimates are updated in reversed fashion.\n",
        "    '''\n",
        "\n",
        "  def reset(self):\n",
        "    self.states = []\n",
        "\n",
        "  def savePolicy(self):\n",
        "    fw = open('new_policy_'+str(self.name),'wb')\n",
        "    pickle.dump(self.states_value,fw)\n",
        "    fw.close()\n",
        "\n",
        "  def loadPolicy(self,file):\n",
        "    fr = open(file,'rb')\n",
        "    self.states_value=pickle.load(fr)\n",
        "    fr.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "2mTk-oc-gm-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HumanPlayer():\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "\n",
        "  def chooseAction(self, positions):\n",
        "    while True:\n",
        "      row = int(input(\"Input your action row:\"))\n",
        "      col = int(input(\"Input your action column:\"))\n",
        "      action = (row, col)\n",
        "      if action in positions:\n",
        "        return action\n",
        "\n",
        "  def addState(self, state):\n",
        "    pass\n",
        "\n",
        "  def feedReward(self, reward):\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "NSjjRcS-pRn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "DN5j1JYzqgBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # training\n",
        "    p1 = Player(\"p1\")\n",
        "    p2 = Player(\"p2\")\n",
        "\n",
        "    st = State(p1, p2)\n",
        "    print(\"training...\")\n",
        "    st.play(10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BjpTGZwqerx",
        "outputId": "1f9fec46-2de6-4524-d3ef-661b842fed05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training...\n",
            "agent_1: {}\n",
            "agent_2: {}\n",
            "agent_1: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "# play with human\n",
        "    p1 = Player(\"computer\", exp_rate=0)\n",
        "    p1.loadPolicy(\"policy_p1\")\n",
        "\n",
        "    p2 = HumanPlayer(\"human\")\n",
        "\n",
        "    st = State(p1, p2)\n",
        "    st.play2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvDtzgmmq4zO",
        "outputId": "05e44881-c9b7-4718-e2c7-d61f3ad7c846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------\n",
            "|  |   |   | \n",
            "-------------\n",
            "|  | x |   | \n",
            "-------------\n",
            "|  |   |   | \n",
            "-------------\n",
            "Input your action row:0\n",
            "Input your action column:0\n",
            "-------------\n",
            "|o |   |   | \n",
            "-------------\n",
            "|  | x |   | \n",
            "-------------\n",
            "|  |   |   | \n",
            "-------------\n",
            "-------------\n",
            "|o | x |   | \n",
            "-------------\n",
            "|  | x |   | \n",
            "-------------\n",
            "|  |   |   | \n",
            "-------------\n",
            "Input your action row:2\n",
            "Input your action column:1\n",
            "-------------\n",
            "|o | x |   | \n",
            "-------------\n",
            "|  | x |   | \n",
            "-------------\n",
            "|  | o |   | \n",
            "-------------\n",
            "-------------\n",
            "|o | x |   | \n",
            "-------------\n",
            "|x | x |   | \n",
            "-------------\n",
            "|  | o |   | \n",
            "-------------\n",
            "Input your action row:1\n",
            "Input your action column:2\n",
            "-------------\n",
            "|o | x |   | \n",
            "-------------\n",
            "|x | x | o | \n",
            "-------------\n",
            "|  | o |   | \n",
            "-------------\n",
            "-------------\n",
            "|o | x | x | \n",
            "-------------\n",
            "|x | x | o | \n",
            "-------------\n",
            "|  | o |   | \n",
            "-------------\n",
            "Input your action row:2\n",
            "Input your action column:0\n",
            "-------------\n",
            "|o | x | x | \n",
            "-------------\n",
            "|x | x | o | \n",
            "-------------\n",
            "|o | o |   | \n",
            "-------------\n",
            "-------------\n",
            "|o | x | x | \n",
            "-------------\n",
            "|x | x | o | \n",
            "-------------\n",
            "|o | o | x | \n",
            "-------------\n",
            "tie!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nxeHsgtwCns6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "1ccc4872-42f2-4ec7-a81d-32a81035f6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-7355594264dc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the pickle format file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python pkl-to-json.py policy_p1"
      ],
      "metadata": {
        "id": "WFm-Hn_IOyqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRY0GU-WPMxd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}